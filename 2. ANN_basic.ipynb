{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 퍼셉트론 활성화 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = lambda x: 1/(1 + np.exp(-x.clip(-708, 709)))\n",
    "logistic_prime = lambda x: 2 * logistic(x) * (1 - logistic(x))\n",
    "\n",
    "relu = lambda x: np.maximum(0, x)\n",
    "relu_prime = lambda x: (x > 0).astype(float)\n",
    "\n",
    "tanh = lambda x: np.tanh(x)\n",
    "tanh_prime = lambda x: logistic_prime(2*x) * 2\n",
    "\n",
    "elu = lambda x: np.where(x>0, x, np.exp(x)-1)\n",
    "elu_prime = lambda x: np.where(x>0, 1, np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTFUNC = {\n",
    "    'logistic': (logistic, logistic_prime),\n",
    "    'relu': (relu, relu_prime),\n",
    "    'tanh' : (tanh, tanh_prime),\n",
    "    'elu' : (elu, elu_prime)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 손실함수 $L(Y, \\hat{Y})$ (학습오차)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual (Y, Y_hat):\n",
    "    '''L = - Y log Y_hat - (1-Y) log (1-Y_hat)'''\n",
    "    eps = np.finfo(float).eps\n",
    "    return - (( Y @ np.log(Y_hat.clip(eps).T)) + ((1-Y) @ np.log(Y_hat.clip(eps).T))) / Y.shape[1]\n",
    "    #return -(np.dot(Y, np.log(Y_hat.clip(eps)).T) + np.dot(1-Y, np.log((1-Y_hat).clip(eps)).T)) / Y.shape[1]\n",
    "\n",
    "def residual_prime (Y, Y_hat):\n",
    "    '''dY/dY_hat'''\n",
    "    eps = np.finfo(float).eps\n",
    "    return - np.divide(Y, Y_hat.clip(eps)) + np.divide(1-Y, (1-Y_hat).clip(eps))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSSFUNC = {\n",
    "        'residual': (residual, residual_prime)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.39322387, 0.39322387, 0.        , 0.01329611, 0.09035332,\n",
       "       0.01329611])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ACTFUNC['logistic'][1](np.array([-1, 1, 94, 5, -3, -5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "\n",
    "\n",
    "class ANN:\n",
    "    '''Artificial Neural Network Using Numpy'''\n",
    "\n",
    "    def __init__(self, layersizes, activations, lossfunc='residual'):\n",
    "        \n",
    "        self.layersizes = tuple(layersizes)\n",
    "        self.activations = tuple(activations)\n",
    "        self.lossfunc = lossfunc\n",
    "\n",
    "        assert len(self.layersizes)-1 == len(self.activations), \\\n",
    "            \"NN number of layers and the activation function spec does not match\"\n",
    "        \n",
    "        assert all(f in ACTFUNC for f in activations), \\\n",
    "            \"Unrecognized activation function used\"\n",
    "\n",
    "        assert all(isinstance(n, int) and n>=1 for n in layersizes), \\\n",
    "            \"Only positive integral number of perceptrons is allowed in each layer\"\n",
    "        \n",
    "        assert lossfunc in LOSSFUNC, \\\n",
    "            \"Unrecognized loss function used\"\n",
    "\n",
    "        # parameter, each is a row vector\n",
    "\n",
    "        L = len(self.layersizes)\n",
    "        self.Z = [None] * L\n",
    "        self.w = [None] * L\n",
    "        self.b = [None] * L\n",
    "        self.A = [None] * L\n",
    "        self.dZ = [None] * L\n",
    "        self.dw = [None] * L\n",
    "        self.db = [None] * L\n",
    "        self.dA = [None] * L\n",
    "\n",
    "    def init_nn(self, seed=42):\n",
    "        \"Initial weight create\"\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        sigma = 0.1\n",
    "        for l, (insize, outsize) in enumerate(zip(self.layersizes, self.layersizes[1:]), 1):\n",
    "            self.w[l] = np.random.randn(outsize, insize).clip(-6, 6) * sigma\n",
    "            self.b[l] = np.random.randn(outsize, 1).clip(-6, 6) * sigma\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"Feed forward the NN using existing W and b, and overwrite the result variables A and Z\n",
    "        Args:\n",
    "            X (numpy.ndarray): Input data to feed forward\n",
    "        \"\"\"\n",
    "        self.A[0] = X\n",
    "\n",
    "        for l, funcname in enumerate(self.activations, 1):\n",
    "            # Z = W A + b, with A as output from previous layer\n",
    "            # W is of size rxs and A the size sxn with n the number of data instances, Z the size rxn\n",
    "            # b is rx1 and broadcast to each column of Z\n",
    "            g = ACTFUNC[funcname][0]\n",
    "            self.Z[l] = np.dot(self.W[l], self.A[l-1]) + self.b[l]\n",
    "\n",
    "            self.A[l] = g(self.Z[l])\n",
    "\n",
    "        return self.A[-1]\n",
    "\n",
    "    def backward(self, Y, Y_hat):\n",
    "        \"\"\"Back propagation using NN\"\"\"\n",
    "\n",
    "        assert Y.shape[0] == self.layersizes[-1], \"Output size mismatch NN\"\n",
    "        assert Y.shape == Y_hat.shape, \"Output size mismatch reference(Y_hat)\"\n",
    "\n",
    "        self.dA[-1] = LOSSFUNC[self.lossfunc][1](Y, Y_hat)\n",
    "\n",
    "        for l, funcname in reversed(list(enumerate(self.activations, 1))):\n",
    "\n",
    "            m = self.layersizes[l]\n",
    "            g_prime = ACTFUNC[funcname][1]\n",
    "\n",
    "            self.dZ[l] = self.dA[l] * g_prime(self.Z[l])\n",
    "            self.dw[l] = np.dot(self.dZ[l], self.A[l-1].T) / m\n",
    "            self.db[l] = np.sum(self.dZ[l], axis=1, keepdims=True) / m\n",
    "            self.dA[l-1] = np.dot(self.w[l].T, self.dZ[l])\n",
    "\n",
    "    def update(self, alpha):\n",
    "        \"\"\"Updates W and b\n",
    "        Args:\n",
    "            alpha (float): Learning rate\n",
    "        \"\"\"\n",
    "\n",
    "        for l in range(1, len(self.W)):\n",
    "            self.w[l] -= alpha * self.dw[l]\n",
    "            self.b[l] -= alpha * self.db[l]\n",
    "\n",
    "    def fit(self, X, Y, epochs, alpha, printfreq=0):\n",
    "        \"\"\"Train a NN\n",
    "        Args:\n",
    "            X: input data, of size mxn which n is the number of data instances and m the number of\n",
    "                features\n",
    "            Y: reference output, of size nxm which n is the number of data instances and m the size\n",
    "                of each output\n",
    "            alpha: the learning rate\n",
    "            epochs:\n",
    "        \"\"\"\n",
    "        self.init_nn()\n",
    "        lossfunc = LOSSFUNC[self.lossfunc][0]\n",
    "        # train for each epoch\n",
    "        for j in range(epochs):\n",
    "            self.forward(X)\n",
    "            Y_hat = self.A[-1]\n",
    "            self.backward(Y, Y_hat)\n",
    "            self.update(alpha)\n",
    "            if printfreq and j % printfreq == 0:\n",
    "                loss = float(lossfunc(Y, Y_hat))\n",
    "                print(\"Iteration {} - loss value {}\".format(j, loss))\n",
    "        # report loss value\n",
    "        return lossfunc(Y, Y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0.28960</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.390</td>\n",
       "      <td>72.9</td>\n",
       "      <td>2.7986</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>21.14</td>\n",
       "      <td>19.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0.26838</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.794</td>\n",
       "      <td>70.6</td>\n",
       "      <td>2.8927</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>14.10</td>\n",
       "      <td>18.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0.23912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>6.019</td>\n",
       "      <td>65.3</td>\n",
       "      <td>2.4091</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>12.92</td>\n",
       "      <td>21.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.17783</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.569</td>\n",
       "      <td>73.5</td>\n",
       "      <td>2.3999</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>395.77</td>\n",
       "      <td>15.10</td>\n",
       "      <td>17.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.22438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>6.027</td>\n",
       "      <td>79.7</td>\n",
       "      <td>2.4982</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>14.33</td>\n",
       "      <td>16.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "      <td>22.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "      <td>23.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        CRIM   ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "496  0.28960  0.0   9.69   0.0  0.585  5.390  72.9  2.7986  6.0  391.0   \n",
       "497  0.26838  0.0   9.69   0.0  0.585  5.794  70.6  2.8927  6.0  391.0   \n",
       "498  0.23912  0.0   9.69   0.0  0.585  6.019  65.3  2.4091  6.0  391.0   \n",
       "499  0.17783  0.0   9.69   0.0  0.585  5.569  73.5  2.3999  6.0  391.0   \n",
       "500  0.22438  0.0   9.69   0.0  0.585  6.027  79.7  2.4982  6.0  391.0   \n",
       "501  0.06263  0.0  11.93   0.0  0.573  6.593  69.1  2.4786  1.0  273.0   \n",
       "502  0.04527  0.0  11.93   0.0  0.573  6.120  76.7  2.2875  1.0  273.0   \n",
       "503  0.06076  0.0  11.93   0.0  0.573  6.976  91.0  2.1675  1.0  273.0   \n",
       "504  0.10959  0.0  11.93   0.0  0.573  6.794  89.3  2.3889  1.0  273.0   \n",
       "505  0.04741  0.0  11.93   0.0  0.573  6.030  80.8  2.5050  1.0  273.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  target  \n",
       "496     19.2  396.90  21.14    19.7  \n",
       "497     19.2  396.90  14.10    18.3  \n",
       "498     19.2  396.90  12.92    21.2  \n",
       "499     19.2  395.77  15.10    17.5  \n",
       "500     19.2  396.90  14.33    16.8  \n",
       "501     21.0  391.99   9.67    22.4  \n",
       "502     21.0  396.90   9.08    20.6  \n",
       "503     21.0  396.90   5.64    23.9  \n",
       "504     21.0  393.45   6.48    22.0  \n",
       "505     21.0  396.90   7.88    11.9  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston = load_boston()\n",
    "df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "df['target'] = boston.target\n",
    "\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      24.0\n",
       "1      21.6\n",
       "2      34.7\n",
       "3      33.4\n",
       "4      36.2\n",
       "       ... \n",
       "501    22.4\n",
       "502    20.6\n",
       "503    23.9\n",
       "504    22.0\n",
       "505    11.9\n",
       "Name: target, Length: 506, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = df.target\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('quant')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7b6214bfe3ce19b2336b5ee82fe9739f68968628bd5eb86b17deae08d37c2fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
